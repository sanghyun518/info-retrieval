#!/usr/bin/perl -w

use strict;

use Carp;
use FileHandle;

##########################################################
##  VECTOR1
##
##  Usage:   vector1     (no command line arguments)
##
##  The function &main_loop below gives the menu for the system.
##
##  This is an example program that shows how the core
##  of a vector-based IR engine may be implemented in Perl.
##
##  Some of the functions below are unimplemented, and some
##  are only partially implemented. Suggestions for additions
##  are given below and in the assignment handout.
##
##  You should feel free to modify this program directly,
##  and probably use this as a base for your implemented
##  extensions.  As with all assignments, the range of
##  possible enhancements is open ended and creativity
##  is strongly encouraged.
##########################################################


############################################################
## Program Defaults and Global Variables
############################################################

my $DIR  = ".";
my $HOME = ".";

my $token_docs = "";   # tokenized cacm journals
my $corps_freq = "";   # frequency of each token in the journ.
my $stoplist   = "";   # common uninteresting words
my $titles     = "";   # titles of each article in cacm 

# @doc_vector
#
#   An array of hashes, each array index indicating a particular document's
#   weight "vector". 

my @doc_vector = ( );

# %docs_freq_hash
#
# associative array which holds <token, frequency> pairs where
#
#   token     = a particular word or tag found in the cacm corpus
#   frequency = the total number of times the token appears in
#               the corpus.

my %docs_freq_hash = ( );    

# %corp_freq_hash
#
# associative array which holds <token, frequency> pairs where
#
#   token     = a particular word or tag found in the corpus
#   frequency = the total number of times the token appears per
#               document-- that is a token is counted only once
#               per document if it is present (even if it appears 
#               several times within that document).

my %corp_freq_hash = ( );

# %stoplist_hash
#
# common list of uninteresting words which are likely irrelvant
# to any query.
#
#   Note: this is an associative array to provide fast lookups
#         of these boring words

my %stoplist_hash  = ( );

# @titles_vector
#
# vector of the cacm journal titles. Indexed in order of apperance
# within the corpus.

my @titles_vector      = ( );

my @sensenum           = ( );

my %v_profile1         = ( );
my %v_profile2         = ( );
my %v_sum1             = ( );
my %v_sum2             = ( );

my %LLike              = ( );

my $tank_result        = 0.0;
my $plant_result       = 0.0;
my $pers_result        = 0.0;

my $stemming           = "stemmed";
my $weighting          = "expndecay";
my $lc_modelling       = "bag-of-words";
my $option             = 1;

# start program

&main_loop;

##########################################################
##  INIT_FILES
##
##  This function specifies the names and locations of
##  input files used by the program. 
##
##  Parameters:  
##   * $word ("plant" or "tank" or "perplace")
##   * $type   ("stemmed" or "unstemmed")
##
##  If $type == "stemmed", the filenames are initialized
##  to the versions stemmed with the Porter stemmer, while
##  in the default ("unstemmed") case initializes to files
##  containing raw, unstemmed tokens.
##########################################################

sub init_files {
    my $word = shift;
    
    $token_docs = $DIR . "\/$word";
    $corps_freq = $DIR . "\/$word";
    $titles = $DIR . "\/$word\.titles";
    
    if ("stemmed" eq $stemming) {
        $token_docs .= "\.stemmed";
        $corps_freq .= "\.stemmed\.hist";
    }
    else {
        $token_docs .= "\.tokenized";
        $corps_freq .= "\.tokenized\.hist";
    }
}

##########################################################
##  INIT_STOPLIST
##
##  This function loads common words (stop list).
##########################################################

sub init_stoplist {
    %stoplist_hash  = ( );
    
    $stoplist = $DIR . "\/common_words";
    
    if ("stemmed" eq (shift || "")) {
        $stoplist   .= "\.stemmed";
    }
    
    my $stoplist_fh   = new FileHandle $stoplist  , "r"
    or croak "Failed $stoplist";
    
    my $line = undef;
    
    while (defined( $line = <$stoplist_fh> )) {

        chomp $line;
        $stoplist_hash{ $line } = 1;
    }
}

##########################################################
##  INIT_CORP_FREQ 
##
##  This function reads in corpus and document frequencies from
##  the provided histogram file for both the document set
##  and the query set. This information will be used in
##  term weighting.
##
##  It also initializes the arrays representing the stoplist,
##  title list and relevance of document given query.
##########################################################

sub init_corp_freq {
    %docs_freq_hash = ( );
    %corp_freq_hash = ( );
    @titles_vector  = ( );

    my $corps_freq_fh = new FileHandle $corps_freq, "r" 
    or croak "Failed $corps_freq";
    
    my $titles_fh     = new FileHandle $titles    , "r"
    or croak "Failed $titles";

    my $line = undef;

    while (defined( $line = <$corps_freq_fh> )) {

    # so on my computer split will return a first element of undef 
    # if the leading characters are white space, so I eat the white
    # space to insure that the split works right.

    my ($str) = ($line =~ /^\s*(\S.*)/);

    my ($doc_freq,
        $cor_freq, 
        $term    ) = split /\s+/, $str;

    $docs_freq_hash{ $term } = $doc_freq;
    $corp_freq_hash{ $term } = $cor_freq;
    }

    push @titles_vector, "";       # push one empty value onto @titles_vector
                                   # so that indices correspond with title
                                   # numbers.

    while (defined( $line = <$titles_fh> )) {

    chomp $line;
    push @titles_vector, $line;
    }

}

##########################################################
##  INIT_DOC_VECTORS
##
##  This function reads in tokens from the document file.
##  When a .I token is encountered, indicating a document
##  break, a new vector is begun. When individual terms
##  are encountered, they are added to a running sum of
##  term frequencies. To save time and space, it is possible
##  to normalize these term frequencies by inverse document
##  frequency (or whatever other weighting strategy is
##  being used) while the terms are being summed or in
##  a posthoc pass.  The 2D vector array 
##
##    $doc_vector[ $doc_num ]{ $term }
##
##  stores these normalized term weights.
##
##  It is possible to weight different regions of the document
##  differently depending on likely importance to the classification.
##  The relative base weighting factors can be set when 
##  different segment boundaries are encountered.
##
##  This function is currently set up for simple TF weighting.
##########################################################

my @words = ();
my @weights = ();

sub init_doc_vectors {
    @doc_vector = ( );

    my $token_docs_fh = new FileHandle $token_docs, "r"
    or croak "Failed $token_docs";

    my $word    = undef;

    my $doc_num =  0;    # current document number and total docs at end
    my $tweight =  1;    # current weight assigned to document token

    push @doc_vector, { };     # push one empty value onto @doc_vector so that
                               # indices correspond with document numbers
    push @sensenum, 0;
    
    @words = ();
    @weights = ();
    
    my $sense = 0;

    while (defined( $word = <$token_docs_fh> )) {

        chomp $word;

        if (($sense) = $word =~ /^\.I\s\d*\s(\d*)/) {     # indicates start of a new document
        
            if ($doc_num > 0) {
                
                &compute_weights;
                
                if ($lc_modelling eq "adj-separate-LR") {
                    &separate_LR;
                }
                
                for my $ind (0 .. scalar @words - 1) {
                    $doc_vector[$doc_num]{$words[$ind]} += $weights[$ind];
                }
                
            }
            
            @words = ();
            @weights = ();

            push @doc_vector, { };
            push @sensenum, $sense;

            $doc_num++;

            next;
        }
        
        if ($word =~ /[a-zA-Z]/ and ! exists $stoplist_hash{ $word }) {

            if (defined( $docs_freq_hash{ $word } )) {
                push @words, $word;
                push @weights, 1;
            }
            else {
                print "ERROR: Document frequency of zero: ", $word, "\n";
            }
        }
    }
    
    &compute_weights;
    
    if ($lc_modelling eq "adj-separate-LR") {
        &separate_LR;
    }
    
    for my $ind (0 .. scalar @words - 1) {
        $doc_vector[$doc_num]{$words[$ind]} += $weights[$ind];
    }

    # optionally normalize the raw term frequency
    #
    if (not ($weighting eq "bayes")) {
        foreach my $hash (@doc_vector) {
            foreach my $key (keys %{ $hash }) {
                if ($key =~ /^L\-/ || $key =~ /^R\-/) {
                    $hash->{ $key } *= log( $doc_num / $docs_freq_hash{ substr $key, 2 });
                } else {
                    $hash->{ $key } *= log( $doc_num / $docs_freq_hash{ $key });
                }
            }
        }
    }

    return $doc_num;
}

##########################################################
## MAIN_LOOP
##
## Parameters: currently no explicit parameters.
##             performance dictated by user imput.
## 
## Initializes document and query vectors using the
## input files specified in &init_files. Then offers
## a menu and switch to appropriate functions in an
## endless loop.
## 
## Possible extensions at this level:  prompt the user
## to specify additional system parameters, such as the
## similarity function to be used.
##
## Currently, the key parameters to the system (stemmed/unstemmed,
## stoplist/no-stoplist, term weighting functions, vector
## similarity functions) are hardwired in.
##
## Initializing the document vectors is clearly the
## most time consuming section of the program, as 213334 
## to 258429 tokens must be processed, weighted and added
## to dynamically growing vectors.
## 
##########################################################

sub main_loop {
    print <<"EndOfMenu";

    ============================================================
    ==     Welcome to the 600.466 Vector-based IR Engine
    ============================================================

    OPTIONS:
      1 = Tabular result of classification accuracies
      2 = Full list of classification results (stemmed/expndecay/bag-of-words)
      3 = Full list of classification results (simple Bayes model)
      4 = Quit

    ============================================================

EndOfMenu
    ;

    print "Enter Option: ";

    $option = <STDIN>;
    chomp $option;

    exit 0 if $option == 4;
    
    &init_stoplist;
    
    if (not ($option == 2) && not ($option == 3)) {
        print << "EndOfList";

                Position     Local Collocation                Accuracy
    Stemming    Weighting    Modelling                tank    plant    pers/place
    ===============================================================================

EndOfList
    ;
    }
    
    if ($option == 2) {
        &run_all_words;
    } elsif ($option == 3) {
        $stemming        = "stemmed";
        $weighting       = "bayes";
        $lc_modelling    = "bag-of-words";
        
        &run_all_words;
    } else {
        $stemming        = "unstemmed";
        $weighting       = "uniform";
        $lc_modelling    = "bag-of-words";
    
        &run_all_words; &print_result;
        
        $stemming        = "stemmed";
        $weighting       = "expndecay";
        $lc_modelling    = "bag-of-words";
    
        &run_all_words; &print_result;
        
        $stemming        = "unstemmed";
        $weighting       = "expndecay";
        $lc_modelling    = "bag-of-words";
    
        &run_all_words; &print_result;
        
        $stemming        = "unstemmed";
        $weighting       = "expndecay";
        $lc_modelling    = "adj-separate-LR";
    
        &run_all_words; &print_result;
        
        $stemming        = "unstemmed";
        $weighting       = "stepped";
        $lc_modelling    = "adj-separate-LR";
    
        &run_all_words; &print_result;
        
        $stemming        = "unstemmed";
        $weighting       = "custom";
        $lc_modelling    = "adj-separate-LR";
    
        &run_all_words; &print_result;
        
        $stemming        = "stemmed";
        $weighting       = "bayes";
        $lc_modelling    = "bag-of-words";
        
        &run_all_words; &print_result;
    }
}

##########################################################
##  PRINT_RESULT
##
##  Prints single line of result.
##########################################################

sub print_result {
    printf( "    %-9s    %-9s    %-15s          %.2f    %.2f    %.2f\n", 
       $stemming,
       $weighting,
       $lc_modelling,
       $tank_result,
       $plant_result,
       $pers_result);
}

##########################################################
##  RUN_ALL_WORDS
##
##  Runs classification experiment for all words.
##########################################################

sub run_all_words {
    &run_single_word("tank");
    &run_single_word("plant");
    &run_single_word("perplace");
}

##########################################################
##  RUN_SINGLE_WORD
##
##  Runs classification experiment for given single word.
##########################################################

sub run_single_word {
    my $word = shift;
    
    &init_files($word);
    &init_corp_freq;
    &init_doc_vectors;
    
    if ($weighting =~ /bayes/) {
        &init_sum;
        &classify_bayes($word);
    } else {
        &init_average;
        &classify($word);
    }
}

##########################################################
##  COMPUTE_WEIGHTS
##
##  Computes the weights for each "document".
##########################################################

sub compute_weights {
    if ($weighting =~ /expndecay/ || $weighting =~ /bayes/) { 
        &expndecay_weights;
    } elsif ($weighting =~ /stepped/) {
        &stepped_weights;
    } elsif ($weighting =~ /custom/) {
        &custom_weights;
    } else {
        # uniform weights
        # 'weights' are all 1 by default so do nothing here
    }
}

##########################################################
##  GET_AMB_WORD_IDX
##
##  Returns the index at which the ambiguous word occurs.
##########################################################

sub get_amb_word_idx {
    my $amb_word_idx = 0;
    
    for my $idx (0 .. scalar @words - 1) {
        if ($words[$idx] =~ /^\.X\-/) {
            return $amb_word_idx;
        } else {
            $amb_word_idx++;
        }
    }
    
    return $amb_word_idx;
}

##########################################################
##  EXPNDECAY_WEIGHTS
##
##  Computes the exponential decay weights.
##########################################################

sub expndecay_weights {
    my $amb_word_idx = &get_amb_word_idx;
    for my $idx (0 .. scalar @words - 1) {
        if ($idx == $amb_word_idx) {
            $weights[$idx] = 1;
        } else {
            $weights[$idx] = 1.0 / abs($idx - $amb_word_idx);
        }
    }
}

##########################################################
##  STEPPED_WEIGHTS
##
##  Computes the stepped weights.
##########################################################

sub stepped_weights {
    my $amb_word_idx = &get_amb_word_idx;
    for my $idx (0 .. scalar @words - 1) {
        my $diff = abs($idx - $amb_word_idx);
        if ($diff == 0) {
            $weights[$idx] = 1.0;
        } elsif ($diff == 1) {
            $weights[$idx] = 6.0;
        } elsif ($diff == 2 || $diff == 3) {
            $weights[$idx] = 3.0;
        } else {
            $weights[$idx] = 1.0;
        }
    }
}

##########################################################
##  CUSTOM_WEIGHTS
##
##  Computes custom defined weights.
##########################################################

sub custom_weights {
    my $amb_word_idx = &get_amb_word_idx;
    for my $idx (0 .. scalar @words - 1) {
        my $diff = abs($idx - $amb_word_idx);
        if ($diff == 0) {
            $weights[$idx] = 1.0;
        } elsif ($diff == 1) {
            $weights[$idx] = 10.0;
        } elsif ($diff == 2 || $diff == 3 || $diff == 4) {
            $weights[$idx] = 3.0;
        } else {
            $weights[$idx] = 1.0;
        }
    }
}

##########################################################
##  SEPARATE_LR
##
##  Local collocation modelling using adjacent separate LR
##########################################################

sub separate_LR {
    my $amb_word_idx = &get_amb_word_idx;
    if ($amb_word_idx > 0) {
        my $word = $words[$amb_word_idx - 1];
        $words[$amb_word_idx - 1] = "L-$word";
    }
    if ($amb_word_idx < scalar @words - 1) {
        my $word = $words[$amb_word_idx + 1];
        $words[$amb_word_idx + 1] = "R-$word";
    }
}

##########################################################
##  INIT_AVERAGE
##
##  Computes the average (centroid) vectors.
##########################################################

sub init_average {
    %v_profile1 = ();
    %v_profile2 = ();
    
    &average_vectors(1, \%v_profile1);
    &average_vectors(2, \%v_profile2);
}

##########################################################
##  INIT_SUM
##
##  Computes the sum of vectors.
##########################################################

sub init_sum {
    %v_sum1 = ();
    %v_sum2 = ();
    
    &sum_vectors(1, \%v_sum1);
    &sum_vectors(2, \%v_sum2);
}

##########################################################
##  AVERAGE_VECTORS
##
##  Computes the average (centroid) of training vectors
##########################################################

sub average_vectors {
    my $sense = shift;
    my $profile = shift;
    
    my @docs = ();
    
    for (my $i = 1; $i <= 3600; $i++) {
        if ($sensenum[$i] == $sense) {
            push @docs, $doc_vector[$i];
        }
    }
    
    my $sum = 0;
    
    foreach my $hash (@docs) {
        $sum = 0;
        while (my ($key, $value) = each (%{$hash})) {
            $sum += $value * $value;
        }
        $sum = sqrt($sum);
        while (my ($key, $value) = each (%{$hash})) {
            $profile->{$key} += $value / $sum;
        }
    }
    
    my $num = scalar @docs;
    
    foreach my $key (keys %{$profile}) {
        $profile->{$key} /= $num
    }
}

##########################################################
##  SUM_VECTORS
##
##  Computes the sum of vectors for each sense.
##########################################################

sub sum_vectors {
    my $sense = shift;
    my $profile = shift;
    
    my @docs = ();
    
    for (my $i = 1; $i <= 3600; $i++) {
        if ($sensenum[$i] == $sense) {
            push @docs, $doc_vector[$i];
        }
    }
    
    foreach my $hash (@docs) {
        while (my ($key, $value) = each (%{$hash})) {
            $profile->{$key} += $value;
        }
    }
}

##########################################################
##  LOG_LIKELIHOOD
##
##  Computes the log likelihood ratios for v_sum vectors.
##########################################################

sub log_likelihood {
    my $EPSILON = 0.2;
    
    while (my ($term, $value) = each (%v_sum2)) {
        if (exists $v_sum1{$term}) {
            $LLike{$term} = log ( $v_sum1{$term} / $v_sum2{$term} ) 
        } else {
            $LLike{$term} = log ( $EPSILON / $v_sum2{$term} ) 
        }
    }
    
    while (my ($term, $value) = each (%v_sum1)) {
        if (not (exists $v_sum2{$term})) {
            $LLike{$term} = log ( $v_sum1{$term} / $EPSILON);
        }
    }
}

##########################################################
##  CLASSIFY
##
##  Performs classification of 400 test data.
##########################################################

sub classify {
    my $word = shift;
    
    my $correct = 0;
    
    if ($option == 2) {
    print << "EndOfList";
   True Class\tPredicted Class\tSim Diff\tTitle
   ====\t====\t========\t===================
EndOfList
    ;
    }
    
    for (my $ind = 3601; $ind <= 4000; $ind++) {
        my $sim1 = &cosine_sim_a($doc_vector[$ind], \%v_profile1);
        my $sim2 = &cosine_sim_a($doc_vector[$ind], \%v_profile2);

        my $sense = $sim1 > $sim2 ? 1 : 2;
        
        if ($sense == $sensenum[$ind]) {
            $correct++;
        }
        
        if ($option == 2) {
            my $title = substr $titles_vector[$ind], 0, 47;
            print $sensenum[$ind], "\t", $sense, "\t", $sim1 - $sim2, "\t", $title, "\n";
        }
    }
    
    my $accuracy = $correct / 400;
    
    if ($option == 1) {
        if ($word eq "tank") {
            $tank_result = $accuracy;
        } elsif ($word eq "plant") {
            $plant_result = $accuracy;
        } else {
            $pers_result = $accuracy;
        }
    }
    
    if ($option == 2) {
        print "Accuracy on Test Data: ", $accuracy, "\n";
    }
}

##########################################################
##  CLASSIFY_BAYES
##
##  Performs classification of 400 test data using Bayes.
##########################################################

sub classify_bayes {
    my $word = shift;
    
    my $correct = 0;
    
    if ($option == 3) {
    print << "EndOfList";
   True Class\tPredicted Class\tSum of LL\tTitle
   ====\t====\t========\t===================
EndOfList
    ;
    }
    
    &log_likelihood;
    
    my $sumofLL = 0;
    
    for (my $ind = 3601; $ind <= 4000; $ind++) {
        $sumofLL = 0;
        
        foreach my $hash ($doc_vector[$ind]) {
            while (my ($term, $value) = each (%{$hash})) {
                $sumofLL += (exists $LLike{$term} ? $LLike{$term} : 0) * $doc_vector[$ind]{$term};
            }
        }
        
        my $sense = $sumofLL > 0 ? 1 : 2;
        
        if ($sense == $sensenum[$ind]) {
            $correct++;
        }
        
        if ($option == 3) {
            my $title = substr $titles_vector[$ind], 0, 47;
            print $sensenum[$ind], "\t", $sense, "\t", $sumofLL, "\t", $title, "\n";
        }
    }
    
    my $accuracy = $correct / 400;
    
    if ($option == 1) {
        if ($word eq "tank") {
            $tank_result = $accuracy;
        } elsif ($word eq "plant") {
            $plant_result = $accuracy;
        } else {
            $pers_result = $accuracy;
        }
    }
    
    if ($option == 3) {
        print "Accuracy on Test Data: ", $accuracy, "\n";
    }
}

########################################################
## COSINE_SIM_A
## 
## Computes the cosine similarity for two vectors
## represented as associate arrays.
########################################################

sub cosine_sim_a {

    my $vec1 = shift;
    my $vec2 = shift;

    my $num     = 0;
    my $sum_sq1 = 0;
    my $sum_sq2 = 0;

    my @val1 = values %{ $vec1 };
    my @val2 = values %{ $vec2 };

    # determine shortest length vector. This should speed 
    # things up if one vector is considerable longer than
    # the other (i.e. query vector to document vector).

    if ((scalar @val1) > (scalar @val2)) {
    my $tmp  = $vec1;
       $vec1 = $vec2;
       $vec2 = $tmp;
    }

    # calculate the cross product

    my $key = undef;
    my $val = undef;

    while (($key, $val) = each %{ $vec1 }) {
    $num += $val * ($$vec2{ $key } || 0);
    }

    # calculate the sum of squares

    my $term = undef;

    foreach $term (@val1) { $sum_sq1 += $term * $term; }
    foreach $term (@val2) { $sum_sq2 += $term * $term; }

    return &cosine_sim_b($num, $sum_sq1, $sum_sq2);
}


########################################################
##  COSINE_SIM_B
##  
##  This function assumes that the sum of the squares
##  of the term weights have been stored in advance for
##  each document and are passed as arguments.
########################################################

sub cosine_sim_b {

    my $num     = shift;
    my $sum_sq1 = shift;
    my $sum_sq2 = shift;

    return ( $num / sqrt( $sum_sq1 * $sum_sq2 ));
}
